{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7edfdd-9b89-4efa-827d-937f386a3ce1",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ffd9bda-6055-465f-a33c-26b65f5c882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "#Spacy import\n",
    "import spacy\n",
    "\n",
    "#HuggingFace imports\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990be5a0-7e65-41e7-9ba4-8606b62b529d",
   "metadata": {},
   "source": [
    "**Load the dataset for analysis**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f883b93-62b5-42c4-8cf3-160287e7d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregatedProjectsDF = pd.read_csv('projectsAgg2.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb177ccf-960e-43b6-8e27-d6e15cdb67d3",
   "metadata": {},
   "source": [
    "### Testing a spaCy pre-built model (no additional training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2295fc8-a8e2-4008-9bb7-70048fe8a081",
   "metadata": {},
   "source": [
    "**Sample of a description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95da6efc-7339-46d8-a23f-d59903a69c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dés les 1er mois, les enfants sont sensibles à beaucoup de choses. Un tel lieu serait un formidable espace de pédagogie et un véritable lieu d’exploration. Il permettrait de développer leur sensibilité à l’environnement. Ce serait aussi un espace où les professionnels de la petite enfance pourraient s’exprimer, initier des activités créatives. Ce lieu serait accessible aux professionnel(les) de la petite enfance mais aussi aux parents et grands-parents qui le souhaitent. Cet environnement serait un levier indispensable pour répondre aux besoins fondamentaux des bébés (cognitifs, émotionnels, psychologiques et d’expression par le langage). Le contact avec la nature conditionne le développement et le bien-être d'un enfant et d'autant plus pour celui d'un enfant citadin. Un tel endroit favoriserait sa curiosité, sa construction et son épanouissement. Au-delà même du jardinage, le jardin et la nature sont de formidables espaces de créativité. Un tel environnement permet tout à fait l’éveil artistique et culturel des jeunes enfants. La nature fournit en effet aux enfants des opportunités uniques d’apprentissage que ce soit en matière d’engagement, de prise de risques, de découverte, de maîtrise des situations, d’estime de soi et de créativité. Chaque professionnel(les) de la petite enfance y puiseront des idées pour savoir comment intégrer l’éveil à la nature, la créativité et la culture, l'histoire qui lui est liée, dans leur pratique quotidienne. Une variété de projets pourrait s'y développer, tous reliés par le souci de faire vivre ce lieu comme un espace d’ouverture à la créativité, à l'enrichissement de l'environnement du jeune enfant et à tisser des liens dans le quartier. \"La période de la conception aux deux premières années de la vie après la naissance sont déterminantes pour le développement de l’enfant et la santé de l’adulte qu’il deviendra\" : selon le rapport des 1000 premiers jours.Modalités de réalisation - Avis technique :Cette idée est réalisable sur le parc de la Vache. Le service municipal compétent pourra être support pour le choix de la palette végétale et le gros entretien mais il conviendra d’associer un acteur associatif pour la gestion courante et pédagogique. Cette idée sera donc conditionnée à cette organisation.\n"
     ]
    }
   ],
   "source": [
    "print(aggregatedProjectsDF.iloc[88].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c1e97-9fd2-4759-b6f6-a795c9e9ac2a",
   "metadata": {},
   "source": [
    "**Load the model downloaded**  \n",
    "For reference, the package documentation and structures can be found here: https://spacy.io/  \n",
    "It is important to note that they also have a polish model, so the entity recognition/extraction can also be used in the polish datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22be35-fd93-4e5f-8ae4-97112ae8c5fb",
   "metadata": {},
   "source": [
    "**This is a test to check the performance of entity recognition of spaCy package by default**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1dbb95-53dc-4964-b31d-df4cc1e16f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the specific model for the french language\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02133f2-cc2d-4748-9a8c-30c5f48d494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = aggregatedProjectsDF.iloc[88].description\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78453bac-3f2e-4614-adf7-ac980a9f5477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC | s’exprimer\n",
      "MISC | d’\n",
      "MISC | qu’\n",
      "PER | Modalités\n",
      "ORG | Avis\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.label_ ,\"|\", entity.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a480e86-045b-4041-980f-6e1b06692a1e",
   "metadata": {},
   "source": [
    "From this example, we can check that we need to train the model to get better entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79133c16-7ff9-4863-aebc-52c3f3f6b8cc",
   "metadata": {},
   "source": [
    "### Testing with a huggingface pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da26cb-6ee3-4a61-87a3-56f0745a895a",
   "metadata": {},
   "source": [
    "**Loading the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f5d88e-094e-4f45-8d53-2a631c43e97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/samuel-malaga/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d56b7f-4f7a-4d0a-9753-d21e3741a86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity group | LOC | Score--> 0.9806381464004517 | Word--> parc de la Vache \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ner_results = nlp(text)\n",
    "for result in ner_results:\n",
    "    print(f\"Entity group | {result['entity_group']} | Score--> {result['score']} | Word--> {result['word']} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26212269-040e-4619-8205-a050259afd24",
   "metadata": {},
   "source": [
    "We can see that the without a clear instructions, the known entities are not detected and without it a entity extraction of a good quality is not possible at all. This raises a question and also a task to verify, if we structure the entities that we want and guide the LLM via a well structured prompt, can we extract more relevant features?  \n",
    "The inspiration for this comes from these articles:  \n",
    "https://medium.com/@lucasmassucci/entity-recognition-with-llms-and-the-importance-of-prompt-engineering-all-languages-ceda8a7ff3e2  \n",
    "https://medium.com/@manoranjan.rajguru/extracting-entities-from-unstructured-documents-using-large-language-models-f7f2c4d203ee  \n",
    "With this we can try to use more performant models to do it like ollama, deepseek and etc...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e03500-c56d-43ff-ba9d-4954f64b465d",
   "metadata": {},
   "source": [
    "**Using text generation with Deepseek (plus promp engineering)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7de89efb-ddc3-4c77-8bb4-92b189aa20f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "Key ->active.all.allocated || value ->624\n",
      "Key ->active.all.current || value ->344\n",
      "Key ->active.all.freed || value ->280\n",
      "Key ->active.all.peak || value ->344\n",
      "Key ->active.large_pool.allocated || value ->336\n",
      "Key ->active.large_pool.current || value ->187\n",
      "Key ->active.large_pool.freed || value ->149\n",
      "Key ->active.large_pool.peak || value ->187\n",
      "Key ->active.small_pool.allocated || value ->288\n",
      "Key ->active.small_pool.current || value ->157\n",
      "Key ->active.small_pool.freed || value ->131\n",
      "Key ->active.small_pool.peak || value ->157\n",
      "Key ->active_bytes.all.allocated || value ->4242541056\n",
      "Key ->active_bytes.all.current || value ->3752709632\n",
      "Key ->active_bytes.all.freed || value ->489831424\n",
      "Key ->active_bytes.all.peak || value ->3752709632\n",
      "Key ->active_bytes.large_pool.allocated || value ->4165074944\n",
      "Key ->active_bytes.large_pool.current || value ->3751936000\n",
      "Key ->active_bytes.large_pool.freed || value ->413138944\n",
      "Key ->active_bytes.large_pool.peak || value ->3751936000\n",
      "Key ->active_bytes.small_pool.allocated || value ->77466112\n",
      "Key ->active_bytes.small_pool.current || value ->773632\n",
      "Key ->active_bytes.small_pool.freed || value ->76692480\n",
      "Key ->active_bytes.small_pool.peak || value ->1619456\n",
      "Key ->allocated_bytes.all.allocated || value ->4242541056\n",
      "Key ->allocated_bytes.all.current || value ->3752709632\n",
      "Key ->allocated_bytes.all.freed || value ->489831424\n",
      "Key ->allocated_bytes.all.peak || value ->3752709632\n",
      "Key ->allocated_bytes.large_pool.allocated || value ->4165074944\n",
      "Key ->allocated_bytes.large_pool.current || value ->3751936000\n",
      "Key ->allocated_bytes.large_pool.freed || value ->413138944\n",
      "Key ->allocated_bytes.large_pool.peak || value ->3751936000\n",
      "Key ->allocated_bytes.small_pool.allocated || value ->77466112\n",
      "Key ->allocated_bytes.small_pool.current || value ->773632\n",
      "Key ->allocated_bytes.small_pool.freed || value ->76692480\n",
      "Key ->allocated_bytes.small_pool.peak || value ->1619456\n",
      "Key ->allocation.all.allocated || value ->624\n",
      "Key ->allocation.all.current || value ->344\n",
      "Key ->allocation.all.freed || value ->280\n",
      "Key ->allocation.all.peak || value ->344\n",
      "Key ->allocation.large_pool.allocated || value ->336\n",
      "Key ->allocation.large_pool.current || value ->187\n",
      "Key ->allocation.large_pool.freed || value ->149\n",
      "Key ->allocation.large_pool.peak || value ->187\n",
      "Key ->allocation.small_pool.allocated || value ->288\n",
      "Key ->allocation.small_pool.current || value ->157\n",
      "Key ->allocation.small_pool.freed || value ->131\n",
      "Key ->allocation.small_pool.peak || value ->157\n",
      "Key ->inactive_split.all.allocated || value ->191\n",
      "Key ->inactive_split.all.current || value ->3\n",
      "Key ->inactive_split.all.freed || value ->188\n",
      "Key ->inactive_split.all.peak || value ->21\n",
      "Key ->inactive_split.large_pool.allocated || value ->162\n",
      "Key ->inactive_split.large_pool.current || value ->2\n",
      "Key ->inactive_split.large_pool.freed || value ->160\n",
      "Key ->inactive_split.large_pool.peak || value ->20\n",
      "Key ->inactive_split.small_pool.allocated || value ->29\n",
      "Key ->inactive_split.small_pool.current || value ->1\n",
      "Key ->inactive_split.small_pool.freed || value ->28\n",
      "Key ->inactive_split.small_pool.peak || value ->2\n",
      "Key ->inactive_split_bytes.all.allocated || value ->936665088\n",
      "Key ->inactive_split_bytes.all.current || value ->5386752\n",
      "Key ->inactive_split_bytes.all.freed || value ->931278336\n",
      "Key ->inactive_split_bytes.all.peak || value ->62231040\n",
      "Key ->inactive_split_bytes.large_pool.allocated || value ->857881600\n",
      "Key ->inactive_split_bytes.large_pool.current || value ->4063232\n",
      "Key ->inactive_split_bytes.large_pool.freed || value ->853818368\n",
      "Key ->inactive_split_bytes.large_pool.peak || value ->60686336\n",
      "Key ->inactive_split_bytes.small_pool.allocated || value ->78783488\n",
      "Key ->inactive_split_bytes.small_pool.current || value ->1323520\n",
      "Key ->inactive_split_bytes.small_pool.freed || value ->77459968\n",
      "Key ->inactive_split_bytes.small_pool.peak || value ->2091008\n",
      "Key ->max_split_size || value ->-1\n",
      "Key ->num_alloc_retries || value ->2\n",
      "Key ->num_device_alloc || value ->101\n",
      "Key ->num_device_free || value ->0\n",
      "Key ->num_ooms || value ->2\n",
      "Key ->num_sync_all_streams || value ->8\n",
      "Key ->oversize_allocations.allocated || value ->0\n",
      "Key ->oversize_allocations.current || value ->0\n",
      "Key ->oversize_allocations.freed || value ->0\n",
      "Key ->oversize_allocations.peak || value ->0\n",
      "Key ->oversize_segments.allocated || value ->0\n",
      "Key ->oversize_segments.current || value ->0\n",
      "Key ->oversize_segments.freed || value ->0\n",
      "Key ->oversize_segments.peak || value ->0\n",
      "Key ->requested_bytes.all.allocated || value ->4176726565\n",
      "Key ->requested_bytes.all.current || value ->3752431652\n",
      "Key ->requested_bytes.all.freed || value ->424294913\n",
      "Key ->requested_bytes.all.peak || value ->3752431652\n",
      "Key ->requested_bytes.large_pool.allocated || value ->4099261440\n",
      "Key ->requested_bytes.large_pool.current || value ->3751658496\n",
      "Key ->requested_bytes.large_pool.freed || value ->347602944\n",
      "Key ->requested_bytes.large_pool.peak || value ->3751658496\n",
      "Key ->requested_bytes.small_pool.allocated || value ->77465125\n",
      "Key ->requested_bytes.small_pool.current || value ->773156\n",
      "Key ->requested_bytes.small_pool.freed || value ->76691969\n",
      "Key ->requested_bytes.small_pool.peak || value ->1618980\n",
      "Key ->reserved_bytes.all.allocated || value ->3758096384\n",
      "Key ->reserved_bytes.all.current || value ->3758096384\n",
      "Key ->reserved_bytes.all.freed || value ->0\n",
      "Key ->reserved_bytes.all.peak || value ->3758096384\n",
      "Key ->reserved_bytes.large_pool.allocated || value ->3755999232\n",
      "Key ->reserved_bytes.large_pool.current || value ->3755999232\n",
      "Key ->reserved_bytes.large_pool.freed || value ->0\n",
      "Key ->reserved_bytes.large_pool.peak || value ->3755999232\n",
      "Key ->reserved_bytes.small_pool.allocated || value ->2097152\n",
      "Key ->reserved_bytes.small_pool.current || value ->2097152\n",
      "Key ->reserved_bytes.small_pool.freed || value ->0\n",
      "Key ->reserved_bytes.small_pool.peak || value ->2097152\n",
      "Key ->segment.all.allocated || value ->101\n",
      "Key ->segment.all.current || value ->101\n",
      "Key ->segment.all.freed || value ->0\n",
      "Key ->segment.all.peak || value ->101\n",
      "Key ->segment.large_pool.allocated || value ->100\n",
      "Key ->segment.large_pool.current || value ->100\n",
      "Key ->segment.large_pool.freed || value ->0\n",
      "Key ->segment.large_pool.peak || value ->100\n",
      "Key ->segment.small_pool.allocated || value ->1\n",
      "Key ->segment.small_pool.current || value ->1\n",
      "Key ->segment.small_pool.freed || value ->0\n",
      "Key ->segment.small_pool.peak || value ->1\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(type(torch.cuda.memory_stats()))\n",
    "for k,v in torch.cuda.memory_stats().items():\n",
    "    print(f\"Key ->{k} || value ->{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "083a7682-157d-4398-8fec-37c830a5475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 21.38 MiB is free. Including non-PyTorch memory, this process has 3.57 GiB memory in use. Of the allocated memory 3.49 GiB is allocated by PyTorch, and 5.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pipeline = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1180\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1178\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mprocessor\u001b[39m\u001b[33m\"\u001b[39m] = processor\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:114\u001b[39m, in \u001b[36mTextGenerationPipeline.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_model_type(\n\u001b[32m    116\u001b[39m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[32m    117\u001b[39m     )\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._preprocess_params:\n\u001b[32m    119\u001b[39m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[32m    120\u001b[39m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[32m    121\u001b[39m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[32m    122\u001b[39m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/transformers/pipelines/base.py:1016\u001b[39m, in \u001b[36mPipeline.__init__\u001b[39m\u001b[34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1011\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1012\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.device != \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m   1013\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.device, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device < \u001b[32m0\u001b[39m)\n\u001b[32m   1014\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1015\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[38;5;66;03m# If it's a generation pipeline and the model can generate:\u001b[39;00m\n\u001b[32m   1019\u001b[39m \u001b[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001b[39;00m\n\u001b[32m   1020\u001b[39m \u001b[38;5;66;03m# tweaks to the generation config.\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# 2 - load the assistant model if it is passed.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pipeline_calls_generate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.can_generate():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/transformers/modeling_utils.py:3848\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3843\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3844\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3845\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3846\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3847\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master2IS/Stage M1/M2IS-Internship-XAI/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 21.38 MiB is free. Including non-PyTorch memory, this process has 3.57 GiB memory in use. Of the allocated memory 3.49 GiB is allocated by PyTorch, and 5.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "pipeline = pipeline(task=\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ea5db-fc0f-4378-b09e-870efd1820c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
